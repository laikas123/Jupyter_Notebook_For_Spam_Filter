{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "import sys\n",
    "\n",
    "\n",
    "#SPAM PATH HAM PATH \n",
    "SPAM_PATH = os.path.join(\"HamandSpam\", \"Spam\", \"spam\") \n",
    "\n",
    "HAM_PATH = os.path.join(\"HamandSpam\", \"Ham\", \"easy_ham\") \n",
    "\n",
    "#HYPERPARAMETERS \n",
    "REMOVE_PUNCTUATION = True\n",
    "REMOVE_SYMBOLS = True\n",
    "# if False count how many times a word shows up per word\n",
    "# if True count words only once, whether they appear or don't\n",
    "COUNT_SINGLE_INSTANCE = False\n",
    "SET_ALL_WORDS_UPPER_CASE = True\n",
    "COUNT_INVALID_LONE_LETTERS = True\n",
    "CHECK_IF_EMAIL_CONTAINS_MARKUP = True\n",
    "COUNT_UPPER_CASE_WORDS_BEFORE_PROCESSING = True\n",
    "COUNT_ALL_SYMBOL_WORDS_LENGTH_THRESHOLD = [True, 2]\n",
    "#some emails have rather long words, especially spam\n",
    "#this leads to a large dataset when looking at unique words\n",
    "#setting shortest and longest word limits help reduce dataset size\n",
    "REMOVE_WORDS_LENGTH_THRESHOLD = [True, 4, 8]\n",
    "CHECK_MARKUP_TEXT_PRESENCE = True\n",
    "\n",
    "#HELPER FUNCTIONS \n",
    "def removeAllSymbolsFromAString(word):\n",
    "    for letter in word:\n",
    "        if(isSymbol(letter)):\n",
    "            word = word.replace(letter, \"\")\n",
    "\n",
    "    return word\n",
    "\n",
    "\n",
    "def removeAllPunctFromAString(word):\n",
    "    for letter in word:\n",
    "        if(isPunctuation(letter)):\n",
    "            word = word.replace(letter, \"\")\n",
    "\n",
    "    return word\n",
    "    \n",
    "\n",
    "def isPunctuation(letter):\n",
    "    punctuation_text = re.search(r\"([/./?\\,\\'\\-\\—\\!\\:\\;\\[\\]\\(\\)\\/\\\\\\\"])\", letter)\n",
    "    if(punctuation_text is None):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "\n",
    "# checker function for making sure invalid lone letter count only\n",
    "# increments for actual letters\n",
    "def isLetter(letter):\n",
    "    letter_text = re.search(r\"([A-Za-z])\", letter)\n",
    "    if(letter_text is None):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "# capital letter checker, same as above function but don't capitalize\n",
    "# input prior to checking\n",
    "def isCapitalLetter(letter):\n",
    "    letter_text = re.search(r\"([A-Z])\", letter)\n",
    "    if(letter_text is None):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "def isUpperCaseWord(word):\n",
    "    # remove symbols and punctuation since only letters can be capital\n",
    "    word = removeAllPunctFromAString(word)\n",
    "    word = removeAllSymbolsFromAString(word)\n",
    "    # if the word was just symbols and punctuation\n",
    "    # return False if the word was all symbols and punctuation\n",
    "    if(len(word) == 0):\n",
    "        return False\n",
    "    sawACapitalLetter = False\n",
    "    for letter in word:\n",
    "        if(isCapitalLetter(letter) == False):\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def isSymbol(letter):\n",
    "    symbol_text = re.search(r\"([\\?\\/\\\\\\|\\~\\`\\<\\>\\,\\.\\:\\;\\\\\\'\\{\\}\\-\\—\\=\\+\\!\\@\\#\\\\\\$\\%\\^\\&\\*])\", letter)\n",
    "    if(symbol_text is None):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "def isAllSymbolWord(word):\n",
    "    for letter in word:\n",
    "        if(letter == \" \" or letter == \"\"):\n",
    "            return False\n",
    "        if(isSymbol(letter) != True):\n",
    "            return False\n",
    "    return True\n",
    "def isMarkupWord(word):\n",
    "    markup_text = re.search(r\"\\<([A-Za-z0-9_]+)\\>\", word)\n",
    "    if(markup_text is None):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "#keys used in the containers for data that shouldn't be overwritten if an email contains them\n",
    "def isSpecialWord(word):\n",
    "    if(word == \"upperCnt\" or word == \"symbolCnt\" or word == \"loneCnt\" or word == \"markupPrsnt\" or word == \"IsSpam\"):\n",
    "        print(\"Special Word was present\")\n",
    "        sys.Exit()\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def GetDataApplyHyperparameters(spam_path = SPAM_PATH, ham_path = HAM_PATH, norm_dict_path = NORMAL_DICTIONARY_PATH,\n",
    "                               spam_dict_path = SPAM_DICTIONARY_PATH, remove_punctuation = REMOVE_PUNCTUATION,\n",
    "                        remove_symbols = REMOVE_SYMBOLS, count_single_instance = COUNT_SINGLE_INSTANCE,\n",
    "                        set_all_words_upper_case = SET_ALL_WORDS_UPPER_CASE, \n",
    "                         count_invalid_lone_letters = COUNT_INVALID_LONE_LETTERS, \n",
    "                        count_upper_case_words_before_processing = COUNT_UPPER_CASE_WORDS_BEFORE_PROCESSING,\n",
    "                        count_all_symbol_words_length_threshold = COUNT_ALL_SYMBOL_WORDS_LENGTH_THRESHOLD,\n",
    "                        remove_words_length_threshold = REMOVE_WORDS_LENGTH_THRESHOLD, \n",
    "                                check_markup_text_presence = CHECK_MARKUP_TEXT_PRESENCE):\n",
    "    \n",
    "    \n",
    "    onlyfiles = [f for f in listdir(spam_path) if isfile(os.path.join(spam_path, f))]\n",
    "\n",
    "    spam_word_count_per_email_data = {}\n",
    "\n",
    "    email_counter = 1\n",
    "\n",
    "\n",
    "    for filename in onlyfiles:\n",
    "\n",
    "        dictionaryName = \"Email\" + str(email_counter)\n",
    "\n",
    "        spam_word_count_per_email_data[dictionaryName] = {}\n",
    "\n",
    "        encodings = ['utf-8', 'windows-1250', 'windows-1252', 'ascii']\n",
    "        for en in encodings:\n",
    "            try:\n",
    "\n",
    "                file1 = open(os.path.join(spam_path, filename), 'r', encoding=en)\n",
    "\n",
    "                alllines = file1.read()\n",
    "                alllinessplit = alllines.split()\n",
    "\n",
    "                invalidLoneLetterCount = 0\n",
    "                preProccessUpperCaseCount = 0\n",
    "                allSymbolWordsCount = 0\n",
    "\n",
    "                sawMarkupWord = False\n",
    "\n",
    "                for word in alllinessplit:\n",
    "                    if(isinstance(word, str)):\n",
    "\n",
    "                        if(isSpecialWord(word)):\n",
    "                            continue\n",
    "                        \n",
    "                        if(word == \" \" or word == \"\"):\n",
    "                            continue\n",
    "\n",
    "                        #markup check should be done before all processing\n",
    "                        if(check_markup_text_presence):\n",
    "                            if(isMarkupWord(word)):\n",
    "                                sawMarkupWord = True\n",
    "                                continue\n",
    "                                \n",
    "                            \n",
    "                        # counts that need to be done pre proccessing\n",
    "                        if(count_upper_case_words_before_processing):\n",
    "                            if(isUpperCaseWord(word)):\n",
    "                                preProccessUpperCaseCount = preProccessUpperCaseCount + 1\n",
    "                        if(count_all_symbol_words_length_threshold[0]):\n",
    "                            if(len(word) >= count_all_symbol_words_length_threshold[1]):\n",
    "                                if(isAllSymbolWord(word)):\n",
    "                                    allSymbolWordsCount = allSymbolWordsCount + 1\n",
    "                                    continue\n",
    "                                    \n",
    "\n",
    "                        # hyper parameter filters\n",
    "                        wordAfterHyperParameters = word\n",
    "                        if remove_punctuation:\n",
    "                            wordAfterHyperParameters = removeAllPunctFromAString(\n",
    "                                wordAfterHyperParameters)\n",
    "                        if remove_symbols:\n",
    "                            wordAfterHyperParameters = removeAllSymbolsFromAString(\n",
    "                                wordAfterHyperParameters)\n",
    "                        if set_all_words_upper_case:\n",
    "                            wordAfterHyperParameters = wordAfterHyperParameters.upper()\n",
    "\n",
    "                        # count to do after processing\n",
    "                        if len(wordAfterHyperParameters) == 1 and isLetter(\n",
    "                                wordAfterHyperParameters):\n",
    "\n",
    "                            if(count_invalid_lone_letters):\n",
    "                                if(wordAfterHyperParameters != \"i\" and wordAfterHyperParameters != \"I\" and wordAfterHyperParameters != \"a\" and wordAfterHyperParameters != \"A\"):\n",
    "                                    invalidLoneLetterCount = invalidLoneLetterCount + 1\n",
    "\n",
    "                        # count occurrences only for the hyperparameter being set\n",
    "                        if wordAfterHyperParameters in spam_word_count_per_email_data[\n",
    "                                dictionaryName] and count_single_instance == False:\n",
    "                            spam_word_count_per_email_data[dictionaryName][wordAfterHyperParameters] = spam_word_count_per_email_data[\n",
    "                                dictionaryName][wordAfterHyperParameters] + 1\n",
    "                        elif (len(wordAfterHyperParameters) > remove_words_length_threshold[1] and \n",
    "                             len(wordAfterHyperParameters) < remove_words_length_threshold[2]):\n",
    "                            spam_word_count_per_email_data[dictionaryName][wordAfterHyperParameters] = 1\n",
    "                email_counter = email_counter + 1\n",
    "                #if hyperparameters were enabled add their respective values to the container\n",
    "                if(count_invalid_lone_letters):\n",
    "                    spam_word_count_per_email_data[dictionaryName][\"loneCnt\"] = invalidLoneLetterCount\n",
    "                if(count_upper_case_words_before_processing):\n",
    "                    spam_word_count_per_email_data[dictionaryName][\"upperCnt\"] = preProccessUpperCaseCount\n",
    "                if(count_all_symbol_words_length_threshold):\n",
    "                    spam_word_count_per_email_data[dictionaryName][\"symbolCnt\"] = allSymbolWordsCount\n",
    "                if(check_markup_text_presence):\n",
    "                    if(sawMarkupWord):\n",
    "                        spam_word_count_per_email_data[dictionaryName][\"markupPrsnt\"] = 1\n",
    "                    else:\n",
    "                        spam_word_count_per_email_data[dictionaryName][\"markupPrsnt\"] = 0\n",
    "                file1.close()\n",
    "            except UnicodeDecodeError:\n",
    "                print('got unicode error with %s , trying different encoding' % en)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    onlyfiles = [f for f in listdir(ham_path) if isfile(os.path.join(ham_path, f))]\n",
    "\n",
    "\n",
    "    ham_word_count_per_email_data = {}\n",
    "\n",
    "    email_counter = 1\n",
    "\n",
    "    for filename in onlyfiles:\n",
    "\n",
    "        dictionaryName = \"Email\" + str(email_counter)\n",
    "\n",
    "        ham_word_count_per_email_data[dictionaryName] = {}\n",
    "\n",
    "        encodings = ['utf-8', 'windows-1250', 'windows-1252', 'ascii']\n",
    "        for en in encodings:\n",
    "            try:\n",
    "\n",
    "                file1 = open(os.path.join(ham_path, filename), 'r', encoding=en)\n",
    "\n",
    "                alllines = file1.read()\n",
    "                alllinessplit = alllines.split()\n",
    "\n",
    "                invalidLoneLetterCount = 0\n",
    "                preProccessUpperCaseCount = 0\n",
    "                allSymbolWordsCount = 0\n",
    "                \n",
    "                sawMarkupWord = False\n",
    "                \n",
    "                for word in alllinessplit:\n",
    "                    if(isinstance(word, str)):\n",
    "                        \n",
    "                        if(isSpecialWord(word)):\n",
    "                            continue\n",
    "\n",
    "                        if(word == \" \" or word == \"\"):\n",
    "                            continue\n",
    "                            \n",
    "                        #markup check should be done before all processing\n",
    "                        if(check_markup_text_presence):\n",
    "                            if(isMarkupWord(word)):\n",
    "                                sawMarkupWord = True\n",
    "                                continue\n",
    "                                    \n",
    "                        # counts that need to be done pre proccessing\n",
    "                        if(count_upper_case_words_before_processing):\n",
    "                            if(isUpperCaseWord(word)):\n",
    "                                preProccessUpperCaseCount = preProccessUpperCaseCount + 1\n",
    "                        if(count_all_symbol_words_length_threshold[0]):\n",
    "                            if(len(word) > count_all_symbol_words_length_threshold[1]):\n",
    "                                if(isAllSymbolWord(word)):\n",
    "                                    allSymbolWordsCount = allSymbolWordsCount + 1\n",
    "                                    continue\n",
    "                                    \n",
    "\n",
    "                        # hyper parameter filters\n",
    "                        wordAfterHyperParameters = word\n",
    "                        if remove_punctuation:\n",
    "                            wordAfterHyperParameters = removeAllPunctFromAString(\n",
    "                                wordAfterHyperParameters)\n",
    "                        if remove_symbols:\n",
    "                            wordAfterHyperParameters = removeAllSymbolsFromAString(\n",
    "                                wordAfterHyperParameters)\n",
    "                        if set_all_words_upper_case:\n",
    "                            wordAfterHyperParameters = wordAfterHyperParameters.upper()\n",
    "\n",
    "                        # count to do after processing\n",
    "                        if len(wordAfterHyperParameters) == 1 and isLetter(\n",
    "                                wordAfterHyperParameters):\n",
    "\n",
    "                            if(count_invalid_lone_letters):\n",
    "                                if(wordAfterHyperParameters != \"i\" and wordAfterHyperParameters != \"I\" and wordAfterHyperParameters != \"a\" and wordAfterHyperParameters != \"A\"):\n",
    "                                    invalidLoneLetterCount = invalidLoneLetterCount + 1\n",
    "\n",
    "                        # count occurrences only for the hyperparameter being set\n",
    "                        if wordAfterHyperParameters in ham_word_count_per_email_data[\n",
    "                                dictionaryName] and count_single_instance == False:\n",
    "\n",
    "                            ham_word_count_per_email_data[dictionaryName][wordAfterHyperParameters] = ham_word_count_per_email_data[\n",
    "                                dictionaryName][wordAfterHyperParameters] + 1\n",
    "                        elif (len(wordAfterHyperParameters) > remove_words_length_threshold[1] and \n",
    "                             len(wordAfterHyperParameters) < remove_words_length_threshold[2]):\n",
    "                            ham_word_count_per_email_data[dictionaryName][wordAfterHyperParameters] = 1\n",
    "                email_counter = email_counter + 1\n",
    "                if(count_invalid_lone_letters):\n",
    "                    ham_word_count_per_email_data[dictionaryName][\"loneCnt\"] = invalidLoneLetterCount\n",
    "                if(count_upper_case_words_before_processing):\n",
    "                    ham_word_count_per_email_data[dictionaryName][\"upperCnt\"] = preProccessUpperCaseCount\n",
    "                if(count_all_symbol_words_length_threshold[0]):\n",
    "                    ham_word_count_per_email_data[dictionaryName][\"symbolCnt\"] = allSymbolWordsCount\n",
    "                if(check_markup_text_presence):\n",
    "                    if(sawMarkupWord):\n",
    "                        ham_word_count_per_email_data[dictionaryName][\"markupPrsnt\"] = 1\n",
    "                    else:\n",
    "                        ham_word_count_per_email_data[dictionaryName][\"markupPrsnt\"] = 0\n",
    "                file1.close()\n",
    "            except UnicodeDecodeError:\n",
    "                print('got unicode error with %s , trying different encoding' % en)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    print(\"DONE READING ALL EMAIL FILES\",  \"Email counts Spam: \",\n",
    "        len(spam_word_count_per_email_data), \" Ham: \",\n",
    "        len(ham_word_count_per_email_data))\n",
    "\n",
    "    return spam_word_count_per_email_data, ham_word_count_per_email_data\n",
    "\n",
    "\n",
    "spam_word_count_container, ham_word_count_container = GetDataApplyHyperparameters()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# allpossiblewords = []\n",
    "\n",
    "totalWordCount = 0\n",
    "\n",
    "for email in spam_word_count_container:\n",
    "    for word in spam_word_count_container[email]:\n",
    "        if(isMarkupWord(word) or isAllSymbolWord(word) or isPunctuation(word)):\n",
    "            print(word)\n",
    "        totalWordCount = totalWordCount + 1\n",
    "\n",
    "for email in ham_word_count_container:\n",
    "    for word in ham_word_count_container[email]:\n",
    "        if(isMarkupWord(word) or isAllSymbolWord(word) or isPunctuation(word)):\n",
    "            print(word)\n",
    "        totalWordCount = totalWordCount + 1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196181\n"
     ]
    }
   ],
   "source": [
    "print(totalWordCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#to generate random indices when picking words from an email\n",
    "def randomListOfXValuesInRange(high_lim, number_items_to_return):\n",
    "    return_vals = []\n",
    "\n",
    "    if(number_items_to_return <= 0):\n",
    "        return return_vals\n",
    "    \n",
    "    while len(return_vals) < number_items_to_return:\n",
    "        random_num = random.randrange(high_lim)\n",
    "        if(return_vals.count(random_num) == 0):\n",
    "            return_vals.append(random_num)\n",
    "    return return_vals\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def SamplePortionFromEveryEmail(container_spam, container_ham, portion_size):\n",
    "    \n",
    "    possible_words_for_sample = []\n",
    "    \n",
    "    for email in container_spam:\n",
    "        total_words_in_email = len(container_spam[email])\n",
    "        \n",
    "        random_indices = randomListOfXValuesInRange(total_words_in_email, int(round(portion_size*total_words_in_email)))\n",
    "        current_index = 0\n",
    "        for word in container_spam[email]:\n",
    "            if(random_indices.count(current_index) > 0 ):\n",
    "                possible_words_for_sample.append(word)\n",
    "            current_index = current_index + 1\n",
    "            \n",
    "    for email in container_ham:\n",
    "        total_words_in_email = len(container_ham[email])\n",
    "        \n",
    "        random_indices = randomListOfXValuesInRange(total_words_in_email, int(round(portion_size*total_words_in_email)))\n",
    "        current_index = 0\n",
    "        for word in container_ham[email]:\n",
    "            if(random_indices.count(current_index) > 0 ):\n",
    "                possible_words_for_sample.append(word)\n",
    "            current_index = current_index + 1\n",
    "    \n",
    "    return possible_words_for_sample\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataFrameFromSampleWords(sample_words):\n",
    "\n",
    "    #Let 1 = spam and 0 = ham\n",
    "    all_train_rows = []\n",
    "\n",
    "    for email in spam_word_count_container:\n",
    "        single_train_row = []\n",
    "\n",
    "        #check if that email has the word, \n",
    "        #1 if it does, 0 if not\n",
    "        for word in sample_words:\n",
    "            if(word in spam_word_count_container[email]):\n",
    "                single_train_row.append(1)\n",
    "            else:\n",
    "                single_train_row.append(0)\n",
    "        #append 1 for spam for last column which is \"IsSpam\"\n",
    "        single_train_row.append(1)\n",
    "        all_train_rows.append(single_train_row)\n",
    "\n",
    "    for email in ham_word_count_container:\n",
    "        single_train_row = []\n",
    "\n",
    "        #check if that email has the word, \n",
    "        #1 if it does, 0 if not\n",
    "        for word in sample_words:\n",
    "            if(word in ham_word_count_container[email]):\n",
    "                single_train_row.append(1)\n",
    "            else:\n",
    "                single_train_row.append(0)\n",
    "        #append 0 for ham for last column which is \"IsSpam\"\n",
    "        single_train_row.append(0)\n",
    "        all_train_rows.append(single_train_row)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    is_spam_col = \"IsSpam\"\n",
    "    sample_words.append(is_spam_col)\n",
    "\n",
    "\n",
    "    Train_df = pd.DataFrame(all_train_rows, columns = sample_words)\n",
    "\n",
    "    return Train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def calculateProbasForTrainDataFrame(train_df):\n",
    "    X_train = Train_df.drop(\"IsSpam\", axis = 1)\n",
    "    y_train = Train_df[\"IsSpam\"]\n",
    "    forest_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    y_probas_forest = cross_val_predict(forest_clf, X_train, y_train, cv=3,\n",
    "                                    method=\"predict_proba\")\n",
    "    return y_probas_forest\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampleXTimesAndAverageProbas(times_to_sample):\n",
    "    probas_per_sample = []\n",
    "    times_sampled = 0\n",
    "    while times_sampled < times_to_sample:\n",
    "        words_from_sample = SamplePortionFromEveryEmail(spam_word_count_container, ham_word_count_container, 0.1)\n",
    "        df_for_sample = createDataFrameFromSampleWords(words_from_sample)\n",
    "        probas_to_append = calculateProbasForTrainDataFrame(df_for_sample)\n",
    "        probas_per_sample.append(probas_to_append[:,0])\n",
    "        times_sampled = times_sampled + 1\n",
    "    \n",
    "    return probas_per_sample\n",
    "    \n",
    "probas_for_ten_samples = sampleXTimesAndAverageProbas(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 3047)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(probas_for_ten_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcAverageForColumn2DProbaArray(proba_array):\n",
    "    \n",
    "    average_probas_to_return = []\n",
    "    \n",
    "    number_cols = len(proba_array[0])\n",
    "    \n",
    "    col_index = 0\n",
    "    \n",
    "    proba_as_np = np.asarray(proba_array)\n",
    "    \n",
    "    while col_index < number_cols:\n",
    "        current_cols = proba_as_np[:,col_index]\n",
    "        total_cols = len(current_cols)\n",
    "        mean_score = (sum(current_cols)/total_cols)\n",
    "        average_probas_to_return.append(mean_score)\n",
    "        col_index = col_index + 1\n",
    "    \n",
    "\n",
    "    return average_probas_to_return\n",
    "average_of_probas = calcAverageForColumn2DProbaArray(probas_for_ten_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3047,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(np.asarray(average_of_probas)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getYValsFromDataset():\n",
    "    y_train_return = []\n",
    "    for email in spam_word_count_container:\n",
    "          \n",
    "        #append 1 for spam\n",
    "        y_train_return.append(1)\n",
    "        \n",
    "\n",
    "    for email in ham_word_count_container:\n",
    "          \n",
    "        #append 0 for ham\n",
    "        y_train_return.append(0)\n",
    "    \n",
    "    return y_train_return\n",
    "y_train_for_accuracy = getYValsFromDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8800000000000001, 0.0, 0.11000000000000001, 0.049999999999999996, 0.0, 0.11000000000000001, 0.07000000000000002, 0.0, 0.030000000000000006, 0.049999999999999996]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(average_of_probas[0:10])\n",
    "\n",
    "#need to round to 0 or 1\n",
    "rounded_averages = [int(round(val)) for val in average_of_probas]\n",
    "\n",
    "print(rounded_averages[0:10])\n",
    "\n",
    "inverted_rounded_averages = []\n",
    "for value in rounded_averages:\n",
    "    if(value == 1):\n",
    "        inverted_rounded_averages.append(0)\n",
    "    else:\n",
    "        inverted_rounded_averages.append(1)\n",
    "print(inverted_rounded_averages[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9235313423039054"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_train_for_accuracy, inverted_rounded_averages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
